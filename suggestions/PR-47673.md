# PR #47673: language_models: Add image support for Bedrock

_Merged: 2026-02-13T11:41:14Z_
_PR: https://github.com/zed-industries/zed/pull/47673_

## Documentation Suggestions

### Summary
Bedrock language models now support image inputs in conversations and tool results. Users can send images through agents and chat when using compatible Bedrock models (Claude 3+, Amazon Nova Pro/Lite, Meta Llama 3.2 Vision, Mistral Pixtral).

### Suggested Changes

#### 1. docs/src/assistant/model-providers.md
- **Section**: Bedrock section (under "Model Providers")
- **Change**: Add
- **Target keyword**: Bedrock image support
- **Frontmatter**:
  ```yaml
  ---
  title: Configure AI Model Providers in Zed
  description: Connect OpenAI, Anthropic, Google, Ollama, and other language model providers to power Zed's assistant features.
  ---
  ```
- **Links**: 
  - [Assistant overview](/assistant)
  - [Context in the assistant](/assistant/context)
  - [Agent workflows](/agent)
  - Marketing link: [zed.dev/assistant](https://zed.dev/assistant)
- **Suggestion**: 

> **Preview:** This feature is available in Zed Preview. It will be included in the next Stable release.

**Image support**: Bedrock models that support vision (Claude 3 and later, Amazon Nova Pro and Lite, Meta Llama 3.2 Vision models, Mistral Pixtral) can receive images in conversations and tool results. To send an image, use the slash command `/file` followed by an image path, or drag an image directly into the assistant panel.

- **Full-file brand pass**: Required: yes. The existing Bedrock section should be reviewed to ensure it maintains technical grounding and avoids any hedging language. The rest of the file should be checked for consistent voice and that all provider sections meet the rubric.

- **Brand voice scorecard**:

  | Criterion            | Score | Notes |
  | -------------------- | ----- | ----- |
  | Technical Grounding  | 5     | Specific models listed, concrete slash command shown |
  | Natural Syntax       | 5     | Direct instructions, no filler words |
  | Quiet Confidence     | 5     | States capability without hype |
  | Developer Respect    | 5     | Assumes reader knows how to use paths/drag-drop |
  | Information Priority | 5     | Feature first, then how to use it |
  | Specificity          | 5     | Names exact models and input methods |
  | Voice Consistency    | 5     | Matches second-person present tense pattern |
  | Earned Claims        | 5     | No claims, just facts about what works |
  | **TOTAL**            | 40    | Pass |

#### 2. docs/src/assistant/context.md
- **Section**: "Images" section (existing)
- **Change**: Update
- **Target keyword**: image context assistant
- **Frontmatter**:
  ```yaml
  ---
  title: Provide Context to the Assistant
  description: Learn how to give the assistant code, files, diagnostics, terminal output, and images to improve response accuracy.
  ---
  ```
- **Links**:
  - [Model providers](/assistant/model-providers)
  - [Agent workflows](/agent)
  - [Slash commands reference](/assistant/commands)
- **Suggestion**: Update the existing "Images" section to note Bedrock support:

```markdown
## Images

You can add images to assistant messages on providers that support vision models. OpenAI GPT-4o, Anthropic Claude 3 and later, Google Gemini 1.5 and 2.0, and Bedrock vision models (Claude 3+, Amazon Nova Pro and Lite, Meta Llama 3.2 Vision, Mistral Pixtral) all support image inputs.

To add an image, use the `/file` slash command and select an image file, or drag an image from your file system directly into the assistant panel.
```

- **Full-file brand pass**: Required: yes. Review the entire context.md file to remove any hedging language ("simply", "just"), ensure all examples use `{#kb action::Name}` syntax instead of hardcoded keys, and verify consistent second-person voice throughout.

- **Brand voice scorecard**:

  | Criterion            | Score | Notes |
  | -------------------- | ----- | ----- |
  | Technical Grounding  | 5     | Lists specific providers and models |
  | Natural Syntax       | 5     | Clear procedural instructions |
  | Quiet Confidence     | 5     | No unnecessary qualifiers |
  | Developer Respect    | 5     | Shows two methods without over-explaining |
  | Information Priority | 5     | Compatibility first, then methods |
  | Specificity          | 5     | Names exact model families |
  | Voice Consistency    | 5     | Maintains second-person pattern |
  | Earned Claims        | 5     | States facts about model support |
  | **TOTAL**            | 40    | Pass |

### Notes for Reviewer
The code adds `supports_images()` capability detection and handles image encoding for Bedrock models, matching existing patterns in other providers (OpenAI, Anthropic, Google). The documentation changes mirror how image support is described for other providers, maintaining consistency across the docs.
