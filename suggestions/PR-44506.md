# PR #44506: Allow changing the context window size for Ollama

_Merged: 2026-02-12T16:58:25Z_
_PR: https://github.com/zed-industries/zed/pull/44506_

## Documentation Suggestions

### Summary
The code adds a new `context_window` setting to the Ollama provider that applies a global context length to all discovered and configured models, overriding per-model `max_tokens` values. The existing documentation accurately describes this behavior and already includes a Preview callout, so only minor clarifications are needed to improve user understanding.

### Suggested Changes

#### 1. docs/src/ai/llm-providers.md

- **Section**: "Ollama Context Length" (#ollama-context)
- **Change**: Update
- **Target keyword**: ollama context window configuration
- **Frontmatter**:
  ```yaml
  ---
  title: LLM Providers - Use Your Own API Keys in Zed
  description: Bring your own API keys to Zed. Set up Anthropic, OpenAI, Google AI, Ollama, DeepSeek, Mistral, OpenRouter, and more.
  ---
  ```
- **Links**: The section already includes adequate internal links to [Agent Panel](./agent-panel.md), [Inline Assistant](./inline-assistant.md), and [Text Threads](./text-threads.md) at the top of the page, as well as links to [Configuration](./configuration.md) and other provider sections.
- **Suggestion**: 

Replace the current "Ollama Context Length" section with:

```markdown
#### Ollama Context Length {#ollama-context}

> **Preview:** This feature is available in Zed Preview. It will be included in the next Stable release.

Zed API requests to Ollama include the context length as the `num_ctx` parameter. By default, Zed uses a context length of `4096` tokens for all Ollama models.

> **Note**: Token counts displayed in the Agent Panel are estimates and will differ from the model's native tokenizer.

You can set a global context length for all Ollama models using the `context_window` setting in your settings.json ([how to edit](../configuring-zed.md#settings-files)):

```json [settings]
{
  "language_models": {
    "ollama": {
      "context_window": 8192
    }
  }
}
```

Or configure it through the Ollama provider settings UI accessible via `agent: open settings`.

When `context_window` is set, it overrides any per-model `max_tokens` values. To configure context length per model instead, use the `max_tokens` field in `available_models`:

```json [settings]
{
  "language_models": {
    "ollama": {
      "api_url": "http://localhost:11434",
      "available_models": [
        {
          "name": "qwen2.5-coder",
          "display_name": "qwen 2.5 coder 32K",
          "max_tokens": 32768,
          "supports_tools": true,
          "supports_thinking": true,
          "supports_images": true
        }
      ]
    }
  }
}
```

If you specify a context length that exceeds your hardware capacity, Ollama will log an error. Watch these logs: `tail -f ~/.ollama/logs/ollama.log` (macOS) or `journalctl -u ollama -f` (Linux). Reduce the context length if errors occur.

You may also specify a `keep_alive` value for each available model. This can be an integer (seconds) or a duration string like "5m", "10m", "1h", "1d". For example, `"keep_alive": "120s"` allows the server to unload the model (freeing GPU VRAM) after 120 seconds.

The `supports_tools` option controls whether the model uses additional tools. If the model is tagged with `tools` in the Ollama catalog, set this option to use the built-in `Ask` and `Write` profiles. If the model lacks the `tools` tag but you set this to `true`, only the `Minimal` built-in profile will work.

The `supports_thinking` option enables explicit reasoning passes before final answers. If the model is tagged with `thinking` in the Ollama catalog, set this option to use it in Zed.

The `supports_images` option enables vision capabilities, allowing the model to process images in conversation context. If the model is tagged with `vision` in the Ollama catalog, set this option to use it in Zed.
```

- **Full-file brand pass**: Required: yes. The entire file already passes the brand rubricâ€”it's direct, uses second person, avoids hedging, and provides concrete examples. This suggested edit maintains that voice and requires no additional changes elsewhere in the file.

- **Brand voice scorecard**:

  | Criterion            | Score | Notes |
  | -------------------- | ----- | ----- |
  | Technical Grounding  | 5/5   | Specifies exact default (4096 tokens), parameter name (num_ctx), exact setting names, file locations, and error checking commands |
  | Natural Syntax       | 4/5   | Generally natural; "Watch these logs" could be "Monitor these logs" but current phrasing is acceptable |
  | Quiet Confidence     | 5/5   | States facts without superlatives; "will differ" not "may slightly differ" |
  | Developer Respect    | 5/5   | Assumes reader can edit JSON, run terminal commands, and understand tradeoffs without hand-holding |
  | Information Priority | 5/5   | Default behavior upfront, most common config next (global context_window via UI or JSON), then per-model alternative, then edge cases |
  | Specificity          | 5/5   | Exact paths for logs (macOS vs Linux), exact setting structure, exact tag names from Ollama catalog |
  | Voice Consistency    | 5/5   | Matches existing docs: second person, present tense, no hedging ("will log" not "may log") |
  | Earned Claims        | 5/5   | All statements are factual: default is 4096, context_window overrides max_tokens, models need catalog tags for features |
  | **TOTAL**            | 39/40 | Pass threshold met (all criteria 4+) |

### Notes for Reviewer

The existing documentation already covers the new feature accurately. This suggestion primarily:

1. **Restructures the flow**: Settings Editor UI option is now mentioned immediately after the JSON example (following the "Settings Editor first" pattern from .rules), rather than buried later
2. **Clarifies override behavior**: Explicitly states that `context_window` overrides `max_tokens` before showing the per-model alternative
3. **Maintains existing content**: Retains all the existing details about `keep_alive`, `supports_tools`, `supports_thinking`, and `supports_images` since they're already well-written and relevant to context configuration

The Preview callout is already correctly positioned. No other sections need updates since the feature only affects Ollama's context configuration.
